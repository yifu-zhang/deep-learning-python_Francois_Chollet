{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d2e5d31",
   "metadata": {},
   "source": [
    "# Dogs vs Cats\n",
    "In this section, we'll start by naively training a small convnet on the 2000 training samples, without any regularization, to set a base-line for what can be achieved. This will get us to a classification accuracy of 71%. At that point, the main issue will be overfitting. Then we'll introduce *data augmentation* , a powerful technique for mitigating overfitting in computer vision. By using data augmentation, we'll improve the network to reach an accuracy of 82%.\n",
    "\n",
    "In next section, we'll review two more essential techniques for applying deep learning to small datasets: *feature extraction with a pretrained network* (which will get an accuracy of 90% to 96%) and *fine-tuning a pretrained network* (97%). Together, there three strategies will constitute our future toolbox for tackling the problem of performing image classification with small datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1563ef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191edf78",
   "metadata": {},
   "source": [
    "__The following code does not run twice__\n",
    "\n",
    "Making small dataset folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "190ad0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, pathlib\n",
    "\n",
    "original_dir = pathlib.Path(\"/home/ubuntu/dlp_fc/chpt5/data/kaggle_original_data/train/train\")\n",
    "new_base_dir = pathlib.Path(\"/home/ubuntu/dlp_fc/chpt5/data/cats_vs_dogs_small\")\n",
    "\n",
    "def make_subset(subset_name, start_index, end_index):\n",
    "    for category in (\"cat\", \"dog\"):\n",
    "        dir = new_base_dir / subset_name / category\n",
    "        os.makedirs(dir)\n",
    "        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n",
    "        for fname in fnames:\n",
    "            shutil.copyfile(src=original_dir / fname,\n",
    "                            dst=dir / fname)\n",
    "\n",
    "make_subset(\"train\", start_index=0, end_index=1000)\n",
    "make_subset(\"validation\", start_index=1000, end_index=1500)\n",
    "make_subset(\"test\", start_index=1500, end_index=2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d0f922",
   "metadata": {},
   "source": [
    "This is a balanced __binary-classification__ problem, which means classification accuracy will be an appropriate measure of success.\n",
    "\n",
    "The depth of the feature maps progressively increases in the network (from 32 to 128), whereas the size of the feature maps decreases (from $148^2$ to $7^2$). This's a pattern you'll see in almost all convnets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b098275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "x = layers.experimental.preprocessing.Rescaling(1./255)(inputs)\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23697669",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706b7f4d",
   "metadata": {},
   "source": [
    "Configuring the model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb8286a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15533085",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "Data should be formatted into appropriately preprocessed floating-point tensors before being fed into the network. The steps for getting data into the network are roughly as follows:\n",
    "1. Read the pic files.\n",
    "2. Decode the JPEG content to RGB grids of pixels.\n",
    "3. Convert these into floating-point tensors.\n",
    "4. Rescale the pixels values (between 0 and 255) to the [0, 1] interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de1cc175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 files belonging to 2 classes.\n",
      "Found 1000 files belonging to 2 classes.\n",
      "Found 2000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    new_base_dir / \"train\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "    new_base_dir / \"validation\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    new_base_dir / \"test\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75037b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data batch shape: (32, 180, 180, 3)\n",
      "labels batch shape: (32,)\n"
     ]
    }
   ],
   "source": [
    "for data_batch, labels_batch in train_dataset:\n",
    "    print(\"data batch shape:\", data_batch.shape)\n",
    "    print(\"labels batch shape:\", labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de0e615",
   "metadata": {},
   "source": [
    "Fitting the model using a batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b0261e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "63/63 [==============================] - 8s 109ms/step - loss: 0.7067 - accuracy: 0.5205 - val_loss: 0.6851 - val_accuracy: 0.5010\n",
      "Epoch 2/30\n",
      "63/63 [==============================] - 7s 105ms/step - loss: 0.7172 - accuracy: 0.5570 - val_loss: 0.6887 - val_accuracy: 0.5110\n",
      "Epoch 3/30\n",
      "63/63 [==============================] - 7s 105ms/step - loss: 0.6623 - accuracy: 0.6125 - val_loss: 0.9464 - val_accuracy: 0.5040\n",
      "Epoch 4/30\n",
      "63/63 [==============================] - 7s 104ms/step - loss: 0.6438 - accuracy: 0.6420 - val_loss: 0.6351 - val_accuracy: 0.6200\n",
      "Epoch 5/30\n",
      "63/63 [==============================] - 7s 106ms/step - loss: 0.6141 - accuracy: 0.6735 - val_loss: 0.6343 - val_accuracy: 0.6420\n",
      "Epoch 6/30\n",
      "63/63 [==============================] - 7s 106ms/step - loss: 0.5795 - accuracy: 0.6965 - val_loss: 0.6177 - val_accuracy: 0.6640\n",
      "Epoch 7/30\n",
      "63/63 [==============================] - 7s 104ms/step - loss: 0.5462 - accuracy: 0.7290 - val_loss: 0.6909 - val_accuracy: 0.6670\n",
      "Epoch 8/30\n",
      "63/63 [==============================] - 7s 104ms/step - loss: 0.4985 - accuracy: 0.7655 - val_loss: 0.6003 - val_accuracy: 0.7150\n",
      "Epoch 9/30\n",
      "63/63 [==============================] - 7s 106ms/step - loss: 0.4626 - accuracy: 0.7945 - val_loss: 0.5927 - val_accuracy: 0.6980\n",
      "Epoch 10/30\n",
      "63/63 [==============================] - 7s 105ms/step - loss: 0.4124 - accuracy: 0.8230 - val_loss: 0.6229 - val_accuracy: 0.7150\n",
      "Epoch 11/30\n",
      "63/63 [==============================] - 7s 105ms/step - loss: 0.3757 - accuracy: 0.8275 - val_loss: 0.6070 - val_accuracy: 0.7480\n",
      "Epoch 12/30\n",
      "63/63 [==============================] - 7s 105ms/step - loss: 0.3183 - accuracy: 0.8550 - val_loss: 0.5936 - val_accuracy: 0.7280\n",
      "Epoch 13/30\n",
      "63/63 [==============================] - 7s 106ms/step - loss: 0.2611 - accuracy: 0.8950 - val_loss: 0.7430 - val_accuracy: 0.7380\n",
      "Epoch 14/30\n",
      "63/63 [==============================] - 7s 105ms/step - loss: 0.2293 - accuracy: 0.9140 - val_loss: 0.7704 - val_accuracy: 0.7220\n",
      "Epoch 15/30\n",
      "63/63 [==============================] - 7s 106ms/step - loss: 0.1714 - accuracy: 0.9335 - val_loss: 0.8991 - val_accuracy: 0.7150\n",
      "Epoch 16/30\n",
      "63/63 [==============================] - 7s 106ms/step - loss: 0.1713 - accuracy: 0.9345 - val_loss: 1.0044 - val_accuracy: 0.7100\n",
      "Epoch 17/30\n",
      "63/63 [==============================] - 7s 106ms/step - loss: 0.1048 - accuracy: 0.9605 - val_loss: 1.4391 - val_accuracy: 0.7190\n",
      "Epoch 18/30\n",
      "63/63 [==============================] - 7s 106ms/step - loss: 0.1113 - accuracy: 0.9575 - val_loss: 1.2062 - val_accuracy: 0.7070\n",
      "Epoch 19/30\n",
      "63/63 [==============================] - 7s 105ms/step - loss: 0.1030 - accuracy: 0.9645 - val_loss: 1.0409 - val_accuracy: 0.7450\n",
      "Epoch 20/30\n",
      "63/63 [==============================] - 7s 105ms/step - loss: 0.0860 - accuracy: 0.9685 - val_loss: 1.1936 - val_accuracy: 0.7240\n",
      "Epoch 21/30\n",
      "63/63 [==============================] - 7s 105ms/step - loss: 0.0727 - accuracy: 0.9765 - val_loss: 1.2534 - val_accuracy: 0.7390\n",
      "Epoch 22/30\n",
      "63/63 [==============================] - 7s 105ms/step - loss: 0.0491 - accuracy: 0.9815 - val_loss: 1.5000 - val_accuracy: 0.7300\n",
      "Epoch 23/30\n",
      "63/63 [==============================] - 7s 105ms/step - loss: 0.0531 - accuracy: 0.9835 - val_loss: 1.6628 - val_accuracy: 0.7290\n",
      "Epoch 24/30\n",
      "63/63 [==============================] - 7s 105ms/step - loss: 0.0756 - accuracy: 0.9755 - val_loss: 2.2239 - val_accuracy: 0.7050\n",
      "Epoch 25/30\n",
      "63/63 [==============================] - 7s 107ms/step - loss: 0.0578 - accuracy: 0.9825 - val_loss: 2.4669 - val_accuracy: 0.6730\n",
      "Epoch 26/30\n",
      "63/63 [==============================] - 7s 105ms/step - loss: 0.0603 - accuracy: 0.9835 - val_loss: 1.6759 - val_accuracy: 0.7400\n",
      "Epoch 27/30\n",
      "63/63 [==============================] - 7s 105ms/step - loss: 0.0489 - accuracy: 0.9850 - val_loss: 1.8739 - val_accuracy: 0.7290\n",
      "Epoch 28/30\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.0317 - accuracy: 0.9889"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=30,\n",
    "    validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f11d8a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
