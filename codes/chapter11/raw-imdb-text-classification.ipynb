{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc399353",
   "metadata": {},
   "source": [
    "## Processing raw IMDB text data\n",
    "take a look at the content of these text files. No matter working with what kinds of data, always remeber to inspect what data looks like before diving into modeling it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3288edc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat aclImdb/train/pos/4077_10.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a75ace5",
   "metadata": {},
   "source": [
    "Next, prepare a validation set by setting apart 20% of the training text files in a new directory, aclImdb/val:\n",
    "### Run only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7db4725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random\n",
    "\n",
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "val_dir = base_dir/\"val\"\n",
    "train_dir = base_dir/\"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce122d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(714).shuffle(files)\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname,\n",
    "                    val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bf7719",
   "metadata": {},
   "source": [
    "Create three $Dataset$ objects for training, validation, and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abbd5c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "batch_size = 32\n",
    "train_ds = keras.preprocessing.text_dataset_from_directory(\"aclImdb/train\",\n",
    "                                                           batch_size=batch_size)\n",
    "val_ds = keras.preprocessing.text_dataset_from_directory(\"aclImdb/val\",\n",
    "                                                         batch_size=batch_size)\n",
    "test_ds = keras.preprocessing.text_dataset_from_directory(\"aclImdb/test\",\n",
    "                                                          batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf7735e",
   "metadata": {},
   "source": [
    "Displaying the shapes and dtypes of the first batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "067c6237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    # print(\"inputs[0]:\", inputs[0])\n",
    "    # print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5145c1e4",
   "metadata": {},
   "source": [
    "## Preprocessing words as a set: the bag-of-words approach\n",
    "The simplest way to encode a piece of text for processing by a machine learning model is to discard order and treat it as a set of tokens. You could either look at individual words(unigrams), or try to recover some local order information by looking at groups of consecutive token(N-grams)\n",
    "### Single words (unigrams) with binary encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff68a055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "text_vectorization = TextVectorization(max_tokens=20000,\n",
    "                                       # encode output tokens as binary vectors\n",
    "                                       output_mode=\"binary\")\n",
    "\n",
    "# prepare a dataset that only yields raw text inputs(no label)\n",
    "text_only_train_ds = train_ds.map(lambda x, y:x)\n",
    "\n",
    "# use this dataset to index the dataset vocabulary, via the adapt() method.\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "binary_1gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y)) \n",
    "binary_1gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "binary_1gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3342b1a5",
   "metadata": {},
   "source": [
    "Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9178676f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32, 20000)\n",
      "inputs.dtype: <dtype: 'float32'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
      "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in binary_1gram_train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    \n",
    "    # these vectors consist entirely of ones and zeros\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7801e5",
   "metadata": {},
   "source": [
    "Let's write a reusable model-building function that we're going be using in all of our experiments.\n",
    "### model-building utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e3532bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_model(max_tokens=20000, hidden_dim=16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                 loss=\"binary_crossentropy\",\n",
    "                 metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a4c8e5",
   "metadata": {},
   "source": [
    "Let's train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c51e1efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 20000)]           0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                320016    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "005b9de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 9s 12ms/step - loss: 0.4012 - accuracy: 0.8317 - val_loss: 0.2996 - val_accuracy: 0.8858\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2729 - accuracy: 0.9027 - val_loss: 0.2859 - val_accuracy: 0.8946\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2437 - accuracy: 0.9157 - val_loss: 0.2995 - val_accuracy: 0.8930\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2270 - accuracy: 0.9219 - val_loss: 0.3178 - val_accuracy: 0.8932\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2211 - accuracy: 0.9267 - val_loss: 0.3375 - val_accuracy: 0.8934\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2152 - accuracy: 0.9309 - val_loss: 0.3380 - val_accuracy: 0.8886\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2146 - accuracy: 0.9314 - val_loss: 0.3522 - val_accuracy: 0.8884\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2118 - accuracy: 0.9358 - val_loss: 0.3545 - val_accuracy: 0.8904\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2006 - accuracy: 0.9384 - val_loss: 0.3771 - val_accuracy: 0.8906\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2069 - accuracy: 0.9376 - val_loss: 0.3749 - val_accuracy: 0.8874\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.2906 - accuracy: 0.8880\n",
      "Test acc: 0.888\n"
     ]
    }
   ],
   "source": [
    "callbacks = [keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
    "                                            save_best_only=True)]\n",
    "\n",
    "# call cache() on datasets to cache in the memory: so that we will only do the preprocessing once, during the first\n",
    "# epoch, and we'll reuse the preprocessed texts for the following epochs. This can only be done if the data is small\n",
    "# enough to fit in memory\n",
    "model.fit(binary_1gram_train_ds.cache(),\n",
    "         validation_data=binary_1gram_val_ds.cache(),\n",
    "         epochs=10,\n",
    "         callbacks=callbacks)\n",
    "model = keras.models.load_model(\"binary_1gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6215de45",
   "metadata": {},
   "source": [
    "This gets us to a test accuracy 88.8%. Let's begin\n",
    "### Bigrams with binary encoding\n",
    "\"The cat sat on the mat\" => {\"the\", \"the cat\", \"cat\", \"cat sat\", \"sat\", \"sat on\", \"on\", \"on the\", \"the mat\", \"mat\"}\n",
    "\n",
    "The TextVectorization layer can be configured to return arbitary N-grams by setting $ngrams=N$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd9fcdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(ngrams=2,\n",
    "                                       max_tokens=20000,\n",
    "                                       output_mode=\"binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54403b6f",
   "metadata": {},
   "source": [
    "Training and testing the binary bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36326c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "binary_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "binary_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "binary_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c0ce429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 9s 13ms/step - loss: 0.4079 - accuracy: 0.8309 - val_loss: 0.2851 - val_accuracy: 0.8912\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2762 - accuracy: 0.9040 - val_loss: 0.3058 - val_accuracy: 0.8916\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2395 - accuracy: 0.9220 - val_loss: 0.3017 - val_accuracy: 0.8976\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2273 - accuracy: 0.9286 - val_loss: 0.3062 - val_accuracy: 0.8948\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2061 - accuracy: 0.9331 - val_loss: 0.3380 - val_accuracy: 0.8910\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2177 - accuracy: 0.9343 - val_loss: 0.3538 - val_accuracy: 0.8890\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2124 - accuracy: 0.9351 - val_loss: 0.3557 - val_accuracy: 0.8890\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2048 - accuracy: 0.9373 - val_loss: 0.3818 - val_accuracy: 0.8886\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2013 - accuracy: 0.9378 - val_loss: 0.3927 - val_accuracy: 0.8846\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2078 - accuracy: 0.9391 - val_loss: 0.4062 - val_accuracy: 0.8832\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 0.2828 - accuracy: 0.8937\n",
      "Test acc: 0.894\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "# model.summary()\n",
    "callbacks = [keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
    "                                            save_best_only=True)]\n",
    "model.fit(binary_2gram_train_ds.cache(),\n",
    "          validation_data=binary_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"binary_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d93550",
   "metadata": {},
   "source": [
    "89.4% test accuracy. Turns out local order is pretty important.\n",
    "### Bigarms with TF-IDF encoding\n",
    "we can also add a bit more information to this representation by counting how many times each word or N-gram occurs\n",
    "\n",
    "{\"the\": 2, \"the cat\": 1, \"cat\": 1, \"cat sat\": 1, \"sat\": 1, \"sat on\": 1, \"on\": 1, \"on the\": 1, \"the mat: 1\", \"mat\": 1}\n",
    "\n",
    "Configuring the TextVectorization layer to return token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7dec4ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(ngrams=2,\n",
    "                                       max_tokens=20000,\n",
    "                                       output_mode=\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4846d6",
   "metadata": {},
   "source": [
    "TF-IDF: term frequency, inverse document frequency\n",
    "\n",
    "Configuring the TextVectorizaiton layers to return TF-IDF weighted outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85ecf69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(ngrams=2,\n",
    "                                       max_tokens=20000,\n",
    "                                       output_mode=\"tf-idf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73918d9",
   "metadata": {},
   "source": [
    "Training and testing the TF-IDF bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ad5ae86",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.5024 - accuracy: 0.7659 - val_loss: 0.2908 - val_accuracy: 0.8930\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.3417 - accuracy: 0.8582 - val_loss: 0.2895 - val_accuracy: 0.8914\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.3022 - accuracy: 0.8716 - val_loss: 0.3107 - val_accuracy: 0.8900\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2725 - accuracy: 0.8882 - val_loss: 0.3351 - val_accuracy: 0.8708\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2615 - accuracy: 0.8901 - val_loss: 0.3448 - val_accuracy: 0.8846\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2469 - accuracy: 0.8959 - val_loss: 0.3415 - val_accuracy: 0.8746\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2452 - accuracy: 0.8985 - val_loss: 0.3480 - val_accuracy: 0.8712\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2441 - accuracy: 0.8982 - val_loss: 0.3471 - val_accuracy: 0.8768\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2394 - accuracy: 0.8965 - val_loss: 0.3462 - val_accuracy: 0.8780\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2355 - accuracy: 0.8975 - val_loss: 0.3699 - val_accuracy: 0.8640\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.2900 - accuracy: 0.8883\n",
      "Test acc: 0.888\n"
     ]
    }
   ],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "tfidf_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "tfidf_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "tfidf_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "\n",
    "model = get_model()\n",
    "# model.summary()\n",
    "callbacks = [keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
    "                                            save_best_only=True)]\n",
    "\n",
    "model.fit(tfidf_2gram_train_ds.cache(),\n",
    "         validation_data=tfidf_2gram_val_ds.cache(),\n",
    "         epochs=10,\n",
    "         callbacks=callbacks)\n",
    "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d72b374",
   "metadata": {},
   "source": [
    "This gets us 88.8% test accuracy on the IMDB classification task: it does not seem to be helpful in this case. However, for many text classfication datasets, it would be typical to see 1% increase when using TF-IDF compared to plain binary  encoding.\n",
    "## 11.3.3 Preprocessing words as a sequence: the Sequence Model approach\n",
    "To implement a sequence model, we'd start by representing our input samples as sequence of integers indices. Then you'd map each integer to a vector, to obtain vector sequences. Finally, we'd feed these sequences of vectors into a stack of layers that can cross-correlate feature from adjacent vectors.\n",
    "### A first practical example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e18bc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "text_vectorization = TextVectorization(max_tokens=max_tokens,\n",
    "                                      output_mode=\"int\",\n",
    "                                      output_sequence_length=max_length)\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfd112e",
   "metadata": {},
   "source": [
    "The simplest tool available to convert our integer sequences to vector sequences is to one-hot encode the integers. On top of these one-hot vectors, we'll add a simple bidirectional LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0858369e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "tf.one_hot (TFOpLambda)      (None, None, 20000)       0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 64)                5128448   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 5,128,513\n",
      "Trainable params: 5,128,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = keras.Input(shape=(None, ), dtype=\"int64\")\n",
    "embedded = tf.one_hot(inputs, depth=max_tokens)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "             loss=\"binary_crossentropy\",\n",
    "             metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cba625",
   "metadata": {},
   "source": [
    "Train first basic sequence model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b937476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 460s 725ms/step - loss: 0.5325 - accuracy: 0.7452 - val_loss: 0.3461 - val_accuracy: 0.8682\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 454s 727ms/step - loss: 0.3477 - accuracy: 0.8719 - val_loss: 0.3384 - val_accuracy: 0.8746\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 455s 728ms/step - loss: 0.2783 - accuracy: 0.9050 - val_loss: 0.5942 - val_accuracy: 0.8224\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 463s 741ms/step - loss: 0.2210 - accuracy: 0.9244 - val_loss: 0.3021 - val_accuracy: 0.8802\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 461s 737ms/step - loss: 0.1952 - accuracy: 0.9341 - val_loss: 0.4376 - val_accuracy: 0.8098\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 450s 720ms/step - loss: 0.1705 - accuracy: 0.9442 - val_loss: 0.3507 - val_accuracy: 0.8550\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 449s 719ms/step - loss: 0.1421 - accuracy: 0.9527 - val_loss: 0.3922 - val_accuracy: 0.8764\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 451s 722ms/step - loss: 0.1210 - accuracy: 0.9607 - val_loss: 0.3884 - val_accuracy: 0.8886\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 450s 720ms/step - loss: 0.0971 - accuracy: 0.9706 - val_loss: 0.4349 - val_accuracy: 0.8748\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 449s 719ms/step - loss: 0.0739 - accuracy: 0.9773 - val_loss: 0.4386 - val_accuracy: 0.8614\n",
      "782/782 [==============================] - 321s 410ms/step - loss: 0.3222 - accuracy: 0.8652\n",
      "Test acc: 0.865\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\",\n",
    "                                    save_best_only=True)]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds,\n",
    "          epochs=10, callbacks=callbacks)\n",
    "model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a04d72",
   "metadata": {},
   "source": [
    "At first observation: this model trains very slowly. This is because our inputs are quite large: each input sample is encoded as a matrix of size (600, 20000). Second the model only gets to 87% test accuracy-it doesn't perform nearly as well as our binary unigram model.\n",
    "### Word embeddings\n",
    "What makes a good word-embedding space depends heavily on our task. The importance of certain semantic relationships varies from task to task. It's reasonable to *learn* a new embedding space with every new task. Fortunately, backpropagation makes this easy.\n",
    "\n",
    "11.17 Instantiating an __Embedding__ layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62a23c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes at least two args: # of possible tokens and the dimensionality of the embeddings (here 256)\n",
    "embedding_layer = layers.Embedding(input_dim = max_tokens, output_dim = 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28fddb5",
   "metadata": {},
   "source": [
    "When we instantiate an *Embedding* layer, its weight are initially random, just as with any other layer. During training, these word vectors are gradually adjusted via backpropagation, structuring the space into something the downstream model can exploit. Once fully trained, the embedding space will show a lot of structure - a kind of structure specialized for the specific problem for which we're training our model.\n",
    "\n",
    "Model that uses an *Embedding* layer trained from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28fb5334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 141s 220ms/step - loss: 0.4720 - accuracy: 0.7898 - val_loss: 0.3380 - val_accuracy: 0.8730\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 137s 219ms/step - loss: 0.2987 - accuracy: 0.8907 - val_loss: 0.4581 - val_accuracy: 0.8510\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 136s 218ms/step - loss: 0.2320 - accuracy: 0.9168 - val_loss: 0.3699 - val_accuracy: 0.8770\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 136s 218ms/step - loss: 0.1987 - accuracy: 0.9330 - val_loss: 0.3181 - val_accuracy: 0.8828\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 136s 218ms/step - loss: 0.1601 - accuracy: 0.9470 - val_loss: 0.3229 - val_accuracy: 0.8882\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 136s 218ms/step - loss: 0.1431 - accuracy: 0.9535 - val_loss: 0.3478 - val_accuracy: 0.8640\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 136s 218ms/step - loss: 0.1144 - accuracy: 0.9650 - val_loss: 0.4759 - val_accuracy: 0.8794\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 136s 218ms/step - loss: 0.0958 - accuracy: 0.9694 - val_loss: 0.4483 - val_accuracy: 0.8790\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 136s 218ms/step - loss: 0.0841 - accuracy: 0.9744 - val_loss: 0.5152 - val_accuracy: 0.8706\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 137s 220ms/step - loss: 0.0729 - accuracy: 0.9778 - val_loss: 0.4876 - val_accuracy: 0.8698\n",
      "782/782 [==============================] - 59s 75ms/step - loss: 0.3543 - accuracy: 0.8625\n",
      "Test acc: 0.862\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "# model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru.keras\",\n",
    "                                    save_best_only=True)]\n",
    "\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
    "model = keras.models.load_model(\"embeddings_bidir_gru.keras\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aef0376",
   "metadata": {},
   "source": [
    "### Understanding padding and masking\n",
    "We're using a bidirecitonal RNN: two RNN layers running in parallel, one processing the tokens in their natural order, and the other processing the same tokens in reverse.  The RNN that looks at the tokens in their natural order will spend its last iterations seeing only vectors that encode paddingâ€”possibly for several hundreds of iterations, if the original sentence was short. We need some way to tell the RNN that it should skip these iterations.\n",
    "\n",
    "There's an API fot that: masking. Let's inspect this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ba01ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = layers.Embedding(input_dim = 10, output_dim = 256, mask_zero = True)\n",
    "\n",
    "some_input = [[4, 3, 2, 1, 0, 0, 0],\n",
    "              [5, 4, 3, 2, 1, 0, 0],\n",
    "              [2, 1, 0, 0, 0, 0, 0]]\n",
    "mask = embedding_layer.compute_mask(some_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ce72cea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 7), dtype=bool, numpy=\n",
       "array([[ True,  True,  True,  True, False, False, False],\n",
       "       [ True,  True,  True,  True,  True, False, False],\n",
       "       [ True,  True, False, False, False, False, False]])>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21c65b5",
   "metadata": {},
   "source": [
    "Let's try retraining model with masking enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b66d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = layers.Embedding(input_dim=max_tokens,\n",
    "                            output_dim=256, mask_zero=True)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "# model.summary()\n",
    "callbacks = [keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru_with_masking.keras\",\n",
    "                                             save_best_only=True)]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds,\n",
    "          epochs=10, callbacks=callbacks)\n",
    "model = keras.models.load_model(\"embeddings_bidir_gru_with_masking.keras\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f474d0",
   "metadata": {},
   "source": [
    "### Using pretrained word embeddings\n",
    "Boring and skip, will learn when necessary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
